{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18a97e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory Usage Before Loading Model:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Ensure we are using GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Print GPU info if available\n",
    "if device.type == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Memory Usage Before Loading Model:\")\n",
    "    print(\"Allocated:\", round(torch.cuda.memory_allocated(0) / 1024**3, 1), \"GB\")\n",
    "    print(\"Cached:   \", round(torch.cuda.memory_reserved(0) / 1024**3, 1), \"GB\")\n",
    "\n",
    "# Clear cache before loading the model to prevent memory fragmentation\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model and tokenizer setup\n",
    "# This is a very small model just to demonstrate the code. For reference, a model I would recommend:\n",
    "# https://huggingface.co/neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16\n",
    "# However, it requires a huggingface account and getting approved to access to the model.\n",
    "\n",
    "# For larger scale operations (e.g. annotating 1000s of texts), look into vllm to make the process more optimized.\n",
    "model_id = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model explicitly onto GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Text generation pipeline with explicit device assignment\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=device,  # Ensure correct device is used\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be9fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: How do I print banana in Python? <issue_label=\"Open source\" />\n",
      "<p align='right'><a href='https://github.com/bartlomiejobson07'>Github</a></p> ---\r",
      "template: post-listing\r",
      "title:\"Hacktoberfest\"\r",
      "---   # About the Post \r",
      "\r",
      "   15 Nov 20, Tue.\r\n",
      "#     * Author : <NAME>\r",
      "\r",
      "\r",
      "\t[GitHub](https://www.linkedin.com/) | [Link](mailto:`+{name}@gmail.<EMAIL>`)\r",
      "                             -------------------------      ## What is HackerFest?\r",
      "\r",
      "        **What we will have this year**         ![happy](/static/media/imgs/_3x49d86eefacccadfcecbfdecaeaecfafeebabdfgahjheemqrkrlghhnmrzhaimnz).            If you are new to hackers or would like a refresher from one of our amazingly diverse community then check out what they offer at https://techcommunity.microsoft.com/.                The following links were provided by members and volunteer organisations involved with Hacker Fests around North America (some may require registration before viewable):           ### Link for US National Membership         We welcome any member of Microsoft as well! Please contact us via email if interested!.       ##### Meetup Group & Events         A place where anyone can come together on their own initiative, share experiences etc..              Our meetups take places online so no need to attend physical events here unless it conflicts your work / personal schedule :)               #### Contact Us         Send an emai directly through http:/tillmanorcode.info/#contact-us/, make sure that \"Contact Details...\" section contains details such as username(es) and name of organization responsible for maintaining site. Also add @username!, since some people might want privacy regarding information posted elsewhere.<br/>![meetchain logo][logo] ðŸš€.                             ```javascript```                    // Code snippet for all pages under `src` directory ([example]:./pages/*.tsx, `./**`, `[...`.]) This code was taken from [`my portfolio`](//localhost), powered up with GitHub Pages theme\n"
     ]
    }
   ],
   "source": [
    "# Input prompt\n",
    "prompt = \"How do I print banana in Python?\"\n",
    "\n",
    "# Generate text\n",
    "sequences = text_generator(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.5,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=500,\n",
    ")\n",
    "\n",
    "# Print generated text\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

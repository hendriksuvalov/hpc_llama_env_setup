{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0468ade-ad0e-48d8-86e9-93b38433b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc98296-bd9e-43ae-8c49-25d70574cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6449875-265d-4bfe-8eb7-f2b4f006b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# HF_TOKEN = \"\"\n",
    "# login(HF_TOKEN)  # This logs you in for the session. This is needed for some specific models that require some consent.\n",
    "\n",
    "# import requests\n",
    "# response = requests.get(\"https://huggingface.co\")\n",
    "# print(response.status_code)  # Should print 200 if the connection is successful\n",
    "\n",
    "model_name = \"neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16\"\n",
    "\n",
    "llm = LLM(model=model_name, device=device, max_model_len=16384, tensor_parallel_size=1, enable_prefix_caching=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ffd28-5363-4941-898a-1e9fa60af815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugStopJSON(BaseModel):\n",
    "    drug_stop_phrase: str\n",
    "    reason_for_stopping: str\n",
    "    drug_name: str\n",
    "\n",
    "json_schema = DrugStopJSON.model_json_schema()\n",
    "\n",
    "guided_decoding_params = GuidedDecodingParams(json=json_schema)\n",
    "sampling_params = SamplingParams(guided_decoding=guided_decoding_params, temperature=0, max_tokens=16384)\n",
    "# sampling_params = SamplingParams(temperature=0.5, max_tokens=1000)\n",
    "\n",
    "# Structure the messages list\n",
    "system_prompt = {\n",
    "  \"role\": \"system\", \"content\": \"\"\"You are a highly intelligent medical expert. Your task is to examine Estonian medical texts written by doctors and extract cases where patients have stopped taking their medications or have never started taking them.  \n",
    "\n",
    "Your response must be a JSON object with the keys \\\"drug_stop_phrase\\\", \\\"reason_for_stopping\\\", and \\\"drug_name\\\":  \n",
    "- \\\"drug_stop_phrase\\\" must be the entire sentence or sentences that explicitly mention the patient stopped or never started the drug AND the reason why. Do not truncate this phrase. If necessary, include multiple sentences to ensure completeness. Do not select vague statements that only describe improvement or side effects without confirming that the patient discontinued the drug.  \n",
    "- \\\"reason_for_stopping\\\" should only include the reason why the patient stopped taking the medication, if mentioned. Do not include generic phrases like 'patsient lõpetas ravimi võtmise' that do not have an actual reason.\n",
    "- \\\"drug_name\\\" should only contain the name of the drug, if mentioned.  \n",
    "\n",
    "If the text does not explicitly state that the patient stopped or never started the medication, do not extract it.  \n",
    "Return the text exactly as it appears in the original input. Do not return anything else.\"\"\"\n",
    "}\n",
    "\n",
    "messages_template = [\n",
    "    system_prompt,\n",
    "    # Example 1\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "                  Patsient lõpetas xanaxi võtmise, kartis jääda sõltuvusse\n",
    "                   \n",
    "                                \"\"\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"{\\\"drug_stop_phrase\\\": \\\"lõpetas xanaxi võtmise, kartis jääda sõltuvusse\\\", \\\"reason_for_stopping\\\": \\\"kartis jääda sõltuvusse\\\", \\\"drug_name\\\": \\\"xanax\\\"}\"},\n",
    "    \n",
    "    # Feel free to add more examples in a similar manner\n",
    "]\n",
    "\n",
    "texts = [\"Patsient lõpetas ise ravimite võtmise (MTX), kuna ei tundnud ravimitest efekti\",\n",
    "        \"Patsient lõpetas ravimi võtmise.\",\n",
    "        \"Haiguse anamnees: Haige toodi vastuvõtuosakonda, kus ta käitus agressiivselt. Olevat lõpetanud ravimite võtmise, kuna ei tundnud, et need aitaksid.\"]\n",
    "\n",
    "messages_list = [\n",
    "    messages_template + [{\"role\": \"user\", \"content\": text}]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "# Tokenize all the messages at once\n",
    "prompts = [\n",
    "    tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    for messages in messages_list\n",
    "]\n",
    "\n",
    "# Use all messages as input\n",
    "outputs = llm.generate(\n",
    "    prompts=prompts,  # Pass messages instead of a raw string\n",
    "    sampling_params=sampling_params,\n",
    ")\n",
    "\n",
    "# print(outputs[0].outputs[0].text)\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Input {i}: {texts[i]}\")\n",
    "    print(f\"Output {i}: {output.outputs[0].text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f099ab36-2c53-4ad1-bbf0-07e360d9e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to run texts from a csv\n",
    "\n",
    "texts_file = \"path_to_csv_file.csv\"\n",
    "\n",
    "df = pd.read_csv(texts_file)\n",
    "nr_of_texts = len(df)\n",
    "\n",
    "# Extract the \"anamnesis\" column into a list\n",
    "texts = df[\"anamnesis\"].dropna().tolist()[:nr_of_texts]  # Drop NaN values if any\n",
    "ids = df[\"id\"].dropna().tolist()[:nr_of_texts]\n",
    "\n",
    "print(nr_of_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b9151-530d-4885-a131-dc6828aa13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_list = [\n",
    "    messages_template + [{\"role\": \"user\", \"content\": text}]\n",
    "    for text in texts # only first 100 texts, beware and change\n",
    "]\n",
    "\n",
    "# Tokenize all messages in one go\n",
    "prompts = [\n",
    "    tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    for messages in messages_list\n",
    "]\n",
    "print(len(prompts))\n",
    "\n",
    "# Output for all texts at once (progress bar seems to be bugged)\n",
    "outputs = llm.generate(\n",
    "    prompts=prompts,  # Pass messages instead of a raw string\n",
    "    sampling_params=sampling_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd1dfa-bbd4-4d5b-96bb-72ea25c24e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output_folder/\"\n",
    "run_name = \"llama_date_prompt1\"\n",
    "\n",
    "output_file = output_dir + run_name + \".csv\"\n",
    "\n",
    "# Check that the amount of texts matches ids with the assertion below. \n",
    "# I do a poor mapping of ensuring that texts have the ids, it should be fine if every text\n",
    "# goes through, but basically I assume that they are all in the same order here as when I read them in. \n",
    "\n",
    "# assert len(ids) == len(outputs)\n",
    "\n",
    "output_list = [output.outputs[0].text for output in outputs]\n",
    "\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    # Writing header\n",
    "    writer.writerow([\"id\", \"result\"])  \n",
    "    \n",
    "    # Convert dictionary to JSON string before writing\n",
    "    writer.writerows(zip(ids, (json.dumps(entry, ensure_ascii=False) for entry in output_list)))\n",
    "\n",
    "print(f\"CSV file '{output_file}' has been created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

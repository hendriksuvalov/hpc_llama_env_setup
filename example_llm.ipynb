{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0468ade-ad0e-48d8-86e9-93b38433b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc98296-bd9e-43ae-8c49-25d70574cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e0f15-3f9a-4466-86ca-e3fdd2f272c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973382c-aea7-4284-bc00-9bc25b916721",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "token = \"enda_token_siia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a86c7-a8c3-4511-8862-0386738c3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16\", device=device, max_model_len=8192, tensor_parallel_size=1, enable_prefix_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ffd28-5363-4941-898a-1e9fa60af815",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16')\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=8192)\n",
    "# sampling_params = SamplingParams(temperature=0.5, max_tokens=1000)\n",
    "\n",
    "# Structure the messages list\n",
    "messages = [\n",
    "    # System prompt\n",
    "    {\"role\": \"system\", \"content\": \"You are a highly intelligent conversation bot.\"},\n",
    "    # User prompt\n",
    "    {\"role\": \"user\", \"content\": \"What do you know about Sven Laur?\"}\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "# Use messages as input (if your model supports chat-like input)\n",
    "outputs = llm.generate(\n",
    "    prompts=prompts,  # Pass messages instead of a raw string\n",
    "    sampling_params=sampling_params,\n",
    ")\n",
    "\n",
    "print(outputs[0].outputs[0].text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
